{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44698d73",
   "metadata": {
    "id": "44698d73"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/drive/10FNhJMwKokjIzh9MBnRM3t4Kz9vM1BuC\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# FiftyOne Workshop - Agriculture\n",
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a8a4",
   "metadata": {
    "id": "fbb1a8a4"
   },
   "source": [
    "## üèÜ Learning Objectives\n",
    "- Use Remote Models for your model evaluation\n",
    "- Load predictions in your actual daaset\n",
    "- Use model evaluation Panel for model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c53e89",
   "metadata": {
    "id": "61c53e89"
   },
   "source": [
    "## Requirements\n",
    "### Knowledge\n",
    "- Understanding of image segmentation.\n",
    "- Familiarity with deep learning-based annotation tools.\n",
    "### Installation\n",
    "This installation process has tested with Python 3.11 and 3.10. Run the following commands to install necessary dependencies:\n",
    "```bash\n",
    "pip install fiftyone\n",
    "python -m pip install --upgrade pip wheel setuptools\n",
    "pip install geti-sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EE6EUTEHH00i",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 43733,
     "status": "ok",
     "timestamp": 1754128404199,
     "user": {
      "displayName": "Paula Ramos",
      "userId": "16781678718580955474"
     },
     "user_tz": 240
    },
    "id": "EE6EUTEHH00i",
    "outputId": "69947652-100c-4fed-c508-a3671f1d337e"
   },
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lkpn28DWH1I8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6861,
     "status": "ok",
     "timestamp": 1754128424122,
     "user": {
      "displayName": "Paula Ramos",
      "userId": "16781678718580955474"
     },
     "user_tz": 240
    },
    "id": "lkpn28DWH1I8",
    "outputId": "25f481fe-27d2-4107-be39-bb8cf46f5a6d"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get the appropriate device for model inference.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79079143",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 47416,
     "status": "ok",
     "timestamp": 1754128482232,
     "user": {
      "displayName": "Paula Ramos",
      "userId": "16781678718580955474"
     },
     "user_tz": 240
    },
    "id": "79079143",
    "outputId": "9f295695-c69d-41ff-f707-63f2eddaad4b"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "from fiftyone.utils.coco import COCODetectionDatasetImporter\n",
    "\n",
    "import gdown\n",
    "\n",
    "# Download the coffee dataset from Google Drive\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1bzAxeSXKe3vHx_Py-VQTc1cOgYpChl8s\" # evaluation\n",
    "gdown.download(url, output=\"coffee_evaluation_FO.zip\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qn8vSgEqJCT8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 117396,
     "status": "ok",
     "timestamp": 1754128707241,
     "user": {
      "displayName": "Paula Ramos",
      "userId": "16781678718580955474"
     },
     "user_tz": 240
    },
    "id": "Qn8vSgEqJCT8",
    "outputId": "29eb0623-7770-487b-e7f0-3c83a3fd08f4"
   },
   "outputs": [],
   "source": [
    "!unzip coffee_evaluation_FO.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96663f08",
   "metadata": {
    "id": "96663f08"
   },
   "source": [
    "If dataset is already in disk just use the next cell for loading it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decca2b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1754129140897,
     "user": {
      "displayName": "Paula Ramos",
      "userId": "16781678718580955474"
     },
     "user_tz": 240
    },
    "id": "decca2b9",
    "outputId": "4bf0adb4-864c-4dc2-d46b-22c892e8cc51"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset_viz =fo.Dataset.from_dir(\n",
    "    dataset_dir=\"/content/coffee_evaluation_FO\",\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    name=\"coffee_evaluation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ii9HSNkiNK38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "executionInfo": {
     "elapsed": 9801,
     "status": "ok",
     "timestamp": 1754129157208,
     "user": {
      "displayName": "Paula Ramos",
      "userId": "16781678718580955474"
     },
     "user_tz": 240
    },
    "id": "Ii9HSNkiNK38",
    "outputId": "a2df6d75-7092-425e-e8f4-b10002abf867"
   },
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset_viz, auto=False)\n",
    "session.open_tab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faf9a3a",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------\n",
    "\n",
    "If you want to run the inference, run the rest of the notebook, following the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8818e3a1",
   "metadata": {
    "id": "8818e3a1"
   },
   "source": [
    "## 1. Loading the Dataset\n",
    "\n",
    "For education purposes, use this link in Drive for downloading an upgraded dataset with 100+ annotated unique images.\n",
    "\n",
    "Download the dataset with this [Link](https://drive.google.com/file/d/1aCr00sF2hjLw7hpq3yeXNUvC07TXdQsg/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W9nxsjFhNNVI",
   "metadata": {
    "id": "W9nxsjFhNNVI"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "\n",
    "from fiftyone.utils.coco import COCODetectionDatasetImporter\n",
    "\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "   dataset_type=fo.types.COCODetectionDataset,\n",
    "   dataset_dir=\"./colombian_coffee-dataset_1600\",\n",
    "   data_path=\"images/default\",\n",
    "   labels_path=\"annotations/instances_default.json\",\n",
    "   label_types=\"segmentations\",\n",
    "   label_field=\"categories\",\n",
    "   name=\"coffee\",\n",
    "   include_id=True,\n",
    "   overwrite=True\n",
    ")\n",
    "\n",
    "view = dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6522f96",
   "metadata": {
    "id": "d6522f96"
   },
   "source": [
    "## 2. Preparing the models for inference\n",
    "\n",
    "We are using Intel Geti SDK for runing the inference of the Intel Geti Models. I have download the deplyments of two models I have created 1- Before data quality improvement, 2- After data quality improvement. Please download the deploment folders using these links and unzip them in the same location of this notebook.\n",
    "\n",
    "Model 1: [Link](https://drive.google.com/file/d/1r_Xi89dTlXt9o2Tj0JQXR2XDuRXHXfQB/view?usp=sharing)\n",
    "\n",
    "Model 2: [Link](https://drive.google.com/file/d/1fkq-4ouMV2owzbV7MEOrOuOIA_UPaJUP/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3ec70",
   "metadata": {
    "id": "69a3ec70",
    "outputId": "61ee7cc2-7005-4540-ebf4-5f137705a3da"
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "devices = ie.available_devices\n",
    "\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87a0cd",
   "metadata": {
    "id": "9c87a0cd"
   },
   "source": [
    "After you have the models in your local disk, we can load the models into memory to prepare them for inference. There you can select the device for running the inference, in OpenVINO we have different options. You can setup CPU, GPU, AUTO for auto plugin, and MULTI for runnig the model in multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729a721",
   "metadata": {
    "id": "f729a721",
    "outputId": "601a312d-9fb3-4419-bb93-6b077ed829b5"
   },
   "outputs": [],
   "source": [
    "from geti_sdk.deployment import Deployment\n",
    "\n",
    "deployment_folder_model_selected_1 = \"/home/paula/projects/databricks/sam2/geti_sdk-deployment_57\"\n",
    "deployment_folder_model_selected_2 = \"/home/paula/projects/databricks/sam2/geti_sdk-deployment_90\"\n",
    "\n",
    "deployment1 = Deployment.from_folder(deployment_folder_model_selected_1)\n",
    "deployment2 = Deployment.from_folder(deployment_folder_model_selected_2)\n",
    "\n",
    "#deployment1.load_inference_models(device=\"CPU\")\n",
    "deployment2.load_inference_models(device=\"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8ff50",
   "metadata": {
    "id": "7eb8ff50"
   },
   "source": [
    "For education purposes we will inspect predictions and annotations coming from my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4780eb9a",
   "metadata": {
    "id": "4780eb9a",
    "outputId": "2646bb88-bbcc-4a57-c0a8-13e4e26f4495"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from PIL import Image as PILImage  # Using PIL's Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to process a single sample and inspect the prediction\n",
    "def inspect_prediction_for_sample(sample):\n",
    "    # Get the image path from the sample\n",
    "    image_path = sample.filepath  # Assuming 'filepath' exists in your sample\n",
    "\n",
    "    # Load the image using PIL\n",
    "    image_data = PILImage.open(image_path)  # Open image using PIL\n",
    "\n",
    "    # Convert the PIL image to a numpy array\n",
    "    image_data = np.array(image_data)\n",
    "\n",
    "    # Run the inference on the image\n",
    "    prediction = deployment1.infer(image_data)\n",
    "\n",
    "    # Print the structure of the prediction object by listing its attributes\n",
    "    print(\"Prediction attributes:\", dir(prediction))\n",
    "\n",
    "    # If there are specific attributes, inspect them\n",
    "    # For example, if there is an attribute like 'segmentation'\n",
    "    if hasattr(prediction, 'segmentation'):\n",
    "        print(\"Segmentation Data:\", prediction.segmentation)\n",
    "\n",
    "    # Check if other useful attributes exist\n",
    "    if hasattr(prediction, 'labels'):\n",
    "        print(\"Labels Data:\", prediction.labels)\n",
    "\n",
    "# Select a single sample from the dataset to test (you can pick the first one for simplicity)\n",
    "sample = dataset.first()\n",
    "\n",
    "# Run the function to inspect the prediction for this sample\n",
    "inspect_prediction_for_sample(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36066e59",
   "metadata": {
    "id": "36066e59"
   },
   "source": [
    "Prediction attributes: ['_GET_only_fields', '__annotations__', '__attrs_attrs__', '__attrs_own_setattr__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_add_shape_to_mask', '_identifier_fields', 'annotation_state_per_task', 'annotations', 'append', 'apply_identifier', 'as_mask', 'created', 'deidentify', 'extend', 'feature_vector', 'filter_annotations', 'filter_by_confidence', 'get_by_shape', 'get_label_names', 'get_labels', 'get_result_media_data', 'has_data', 'has_result_media', 'id', 'kind', 'labels_to_revisit_full_scene', 'map_labels', 'maps', 'media_identifier', 'modified', 'overview', 'prepare_for_post', 'resolve_label_names_and_colors', 'resolve_labels_for_result_media', 'to_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4c23a",
   "metadata": {
    "id": "fae4c23a"
   },
   "source": [
    "Annotations: [Annotation(labels=[ScoredLabel(probability=0.7790045142173767, name='immature', color='#00ff30ff', id='674b942ae9005102abfe2b1b', source=None)], shape=Polygon(points=[Point(x=1319, y=696), Point(x=1318, y=697), Point(x=1317, y=697), Point(x=1316, y=698), Point(x=1316, y=700), Point(x=1315, y=701), Point(x=1315, y=702), Point(x=1314, y=703), Point(x=1314, y=704), Point(x=1313, y=705), Point(x=1313, y=706), Point(x=1312, y=707), Point(x=1312, y=709), Point(x=1311, y=710), Point(x=1311, y=713), Point(x=1310, y=714), Point(x=1310, y=720), Point(x=1309, y=721), Point(x=1309, y=725), Point(x=1308, y=726), Point(x=1308, y=727), Point(x=1306, y=729), Point(x=1306, y=730), Point(x=1305, y=731), Point(x=1305, y=732), Point(x=1304, y=733), Point(x=1304, y=736), Point(x=1303, y=737), Point(x=1303, y=739), Point(x=1302, y=740), Point(x=1302, y=742), Point(x=1296, y=748), Point(x=1296, y=749), Point(x=1291, y=754), Point(x=1291, y=755), Point(x=1287, y=759), Point(x=1286, y=759), Point(x=1285, y=760), Point(x=1285, y=761), Point(x=1281, y=765), Point(x=1280, y=765), Point(x=1279, y=766), Point(x=1278, y=766), Point(x=1262, y=782), Point(x=1261, y=782), Point(x=1260, y=783), Point(x=1259, y=783), Point(x=1258, y=784), Point(x=1256, y=784), Point(x=1255, y=785), Point(x=1254, y=785), Point(x=1250, y=789), Point(x=1248, y=789), Point(x=1247, y=790), Point(x=1239, y=790), Point(x=1238, y=791), Point(x=1235, y=791), Point(x=1234, y=792), Point(x=1233, y=792), Point(x=1232, y=793), Point(x=1231, y=793), Point(x=1229, y=795), Point(x=1228, y=795), Point(x=1227, y=796), Point(x=1222, y=796), Point(x=1221, y=797), Point(x=1215, y=797), Point(x=1214, y=798), Point(x=1212, y=798), Point(x=1209, y=801), Point(x=1209, y=802), Point(x=1206, y=805), Point(x=1206, y=812), Point(x=1207, y=813), Point(x=1207, y=814), Point(x=1208, y=815), Point(x=1208, y=816), Point(x=1209, y=817), Point(x=1209, y=818), Point(x=1210, y=819), Point(x=1210, y=820), Point(x=1211, y=821), Point(x=1211, y=822), Point(x=1213, y=824), Point(x=1213, y=825), Point(x=1216, y=828), Point(x=1216, y=829), Point(x=1217, y=830), Point(x=1217, y=831), Point(x=1218, y=832), Point(x=1218, y=833), Point(x=1219, y=833), Point(x=1222, y=836), Point(x=1222, y=837), Point(x=1224, y=839), Point(x=1224, y=840), Point(x=1225, y=840), Point(x=1229, y=844), Point(x=1229, y=845), Point(x=1231, y=847), Point(x=1232, y=847), Point(x=1235, y=850), Point(x=1235, y=851), Point(x=1236, y=852), Point(x=1237, y=852), Point(x=1238, y=853), Point(x=1239, y=853), Point(x=1242, y=856), Point(x=1242, y=857), Point(x=1243, y=858), Point(x=1244, y=858), Point(x=1245, y=859), Point(x=1246, y=859), Point(x=1247, y=860), Point(x=1249, y=860), Point(x=1253, y=864), Point(x=1254, y=864), Point(x=1255, y=865), Point(x=1257, y=865), Point(x=1258, y=866), Point(x=1259, y=866), Point(x=1260, y=867), Point(x=1261, y=867), Point(x=1262, y=868), Point(x=1263, y=868), Point(x=1265, y=870), Point(x=1266, y=870), Point(x=1267, y=871), Point(x=1270, y=871), Point(x=1271, y=872), Point(x=1294, y=872), Point(x=1295, y=871), Point(x=1301, y=871), Point(x=1302, y=870), Point(x=1303, y=870), Point(x=1306, y=867), Point(x=1307, y=867), Point(x=1308, y=866), Point(x=1311, y=866), Point(x=1312, y=865), Point(x=1315, y=865), Point(x=1316, y=864), Point(x=1317, y=864), Point(x=1318, y=863), Point(x=1319, y=863), Point(x=1322, y=860), Point(x=1323, y=860), Point(x=1324, y=859), Point(x=1328, y=859), Point(x=1329, y=858), Point(x=1331, y=858), Point(x=1336, y=853), Point(x=1341, y=853), Point(x=1342, y=852), Point(x=1345, y=852), Point(x=1349, y=848), Point(x=1350, y=848), Point(x=1351, y=847), Point(x=1354, y=847), Point(x=1355, y=846), Point(x=1357, y=846), Point(x=1364, y=839), Point(x=1364, y=838), Point(x=1366, y=836), Point(x=1366, y=835), Point(x=1367, y=834), Point(x=1368, y=834), Point(x=1370, y=832), Point(x=1370, y=831), Point(x=1371, y=830), Point(x=1371, y=827), Point(x=1372, y=826), Point(x=1372, y=825), Point(x=1374, y=823), Point(x=1374, y=822), Point(x=1376, y=820), Point(x=1376, y=816), Point(x=1377, y=815), Point(x=1377, y=765), Point(x=1376, y=764), Point(x=1376, y=759), Point(x=1375, y=758), Point(x=1375, y=756), Point(x=1373, y=754), Point(x=1373, y=753), Point(x=1372, y=752), Point(x=1372, y=750), Point(x=1371, y=749), Point(x=1371, y=744), Point(x=1370, y=743), Point(x=1370, y=740), Point(x=1369, y=739), Point(x=1369, y=738), Point(x=1366, y=735), Point(x=1366, y=734), Point(x=1365, y=733), Point(x=1365, y=731), Point(x=1364, y=730), Point(x=1364, y=728), Point(x=1363, y=727), Point(x=1363, y=726), Point(x=1361, y=724), Point(x=1361, y=723), Point(x=1359, y=721), Point(x=1359, y=719), Point(x=1358, y=718), Point(x=1358, y=716), Point(x=1353, y=711), Point(x=1353, y=710), Point(x=1352, y=709), Point(x=1351, y=709), Point(x=1350, y=708), Point(x=1349, y=708), Point(x=1348, y=707), Point(x=1347, y=707), Point(x=1343, y=703), Point(x=1342, y=703), Point(x=1341, y=702), Point(x=1338, y=702), Point(x=1337, y=701), Point(x=1334, y=701), Point(x=1333, y=700), Point(x=1332, y=700), Point(x=1330, y=698), Point(x=1329, y=698), Point(x=1328, y=697), Point(x=1327, y=697), Point(x=1326, y=696)], type=<ShapeType.POLYGON: 'POLYGON'>), modified=None, id=None, labels_to_revisit=None), Annotation(labels=[ScoredLabel(probability=0.772817075252533, name='immature', color='#00ff30ff', id='674b942ae9005102abfe2b1b', source=None)], shape=Polygon(points=[Point(x=419, y=654), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b41ed",
   "metadata": {
    "id": "d11b41ed"
   },
   "source": [
    "## 3. Generating instance segmentation masks from polygons and bounding boxes\n",
    "\n",
    "This function extracts instance segmentation masks from polygon annotations, combining **detection (bounding boxes)** and **segmentation (masks)** in the same instance using `fo.Detection`.  \n",
    "\n",
    "1. **Load Image** ‚Äì Reads and converts the image to RGB.  \n",
    "2. **Process Annotations** ‚Äì Extracts polygon points, computes bounding boxes, and normalizes coordinates.  \n",
    "3. **Generate Masks** ‚Äì Creates, crops, and resizes binary masks for each annotation.  \n",
    "4. **Save & Return** ‚Äì Stores masks as temp files and returns `fo.Detection` objects, ensuring the bounding box and mask belong to the same instance.  \n",
    "\n",
    "This enables accurate visualization and analysis in FiftyOne, preserving both object localization and shape details.\n",
    "\n",
    "\n",
    "Useful for visualizing or processing segmentation data in FiftyOne.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb44a8",
   "metadata": {
    "id": "27bb44a8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import fiftyone as fo\n",
    "from PIL import Image as PILImage  # Using PIL's Image\n",
    "from tempfile import NamedTemporaryFile\n",
    "import matplotlib.pyplot as plt\n",
    "from geti_sdk.data_models.shapes import Polygon\n",
    "\n",
    "# Function to generate an instance segmentation mask from annotation polygons and bounding boxes\n",
    "def generate_mask_from_polygon_and_bboxes(sample, prediction):\n",
    "    image = cv2.imread(sample.filepath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for compatibility\n",
    "\n",
    "    img_height, img_width = image.shape[:2]  # Get image dimensions\n",
    "    print(f\"Image size: {img_width}x{img_height}\")\n",
    "\n",
    "    detections = []\n",
    "\n",
    "    for annotation in prediction.annotations:\n",
    "\n",
    "        # Ensure that the annotation has a shape attribute and that it's a Polygon\n",
    "        if isinstance(annotation.shape, Polygon):\n",
    "\n",
    "            # Extract the polygon points from the annotation shape\n",
    "            polygon_points = [(point.x, point.y) for point in annotation.shape.points]\n",
    "            polygon_points = np.array(polygon_points, dtype=np.int32)\n",
    "\n",
    "            # Get the label and confidence of the annotation\n",
    "            label = annotation.labels[0].name\n",
    "            confidence = annotation.labels[0].probability\n",
    "\n",
    "            # Calculate the bounding box from the polygon points\n",
    "            x, y, w, h = cv2.boundingRect(polygon_points)\n",
    "            scaled_x = x / img_width\n",
    "            scaled_y = y / img_height\n",
    "            scaled_w = w / img_width\n",
    "            scaled_h = h / img_height\n",
    "            bounding_box = [scaled_x, scaled_y, scaled_w, scaled_h]\n",
    "\n",
    "            # Create the mask for the annotation\n",
    "            mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "            cv2.fillPoly(mask, [polygon_points], 255)  # Fill the polygon area\n",
    "\n",
    "            # Crop the mask to fit the bounding box (bounding box coordinates in pixels)\n",
    "            cropped_mask = mask[y:y + h, x:x + w]\n",
    "\n",
    "            # Resize the cropped mask to fit the bounding box dimensions\n",
    "            mask_resized = cv2.resize(cropped_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            # Verify that the resized mask matches the expected size\n",
    "            print(f\"Mask size: {mask_resized.shape} (expected: {h}x{w})\")\n",
    "\n",
    "            # Save the resized mask as a temporary file\n",
    "            with NamedTemporaryFile(delete=False, suffix='.png') as temp_mask_file:\n",
    "                mask_path = temp_mask_file.name\n",
    "                cv2.imwrite(mask_path, mask_resized)  # Save the cropped and resized mask image\n",
    "\n",
    "\n",
    "            # Create the detection and append it to the detections list\n",
    "            detection = fo.Detection(\n",
    "                label=label,\n",
    "                confidence=confidence,\n",
    "                bounding_box=bounding_box,  # Scaled bounding box\n",
    "                mask_path=mask_path  # Path to the mask image\n",
    "            )\n",
    "            detections.append(detection)\n",
    "\n",
    "    return detections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8253102a",
   "metadata": {
    "id": "8253102a"
   },
   "source": [
    "For education purposes check what is happening in the first or last sample. Then you can apply this to the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766913c9",
   "metadata": {
    "id": "766913c9",
    "outputId": "09a2d7ea-bc56-4a2f-a83f-1598c814a448"
   },
   "outputs": [],
   "source": [
    "# Test on one image\n",
    "sample = dataset.first()  # Get the first sample from the dataset (or pick a specific one)\n",
    "\n",
    "# Load the image as a NumPy array using PIL or OpenCV\n",
    "image_path = sample.filepath  # Path to the image file\n",
    "image_data = PILImage.open(image_path)\n",
    "image_data = np.array(image_data)  # Convert the image to NumPy array\n",
    "\n",
    "# Run inference on the sample (using Geti SDK's inference)\n",
    "prediction = deployment1.infer(image_data)\n",
    "\n",
    "# Generate the segmentation mask and detections using the annotations from the prediction\n",
    "detections = generate_mask_from_polygon_and_bboxes(sample, prediction)\n",
    "\n",
    "# Add the detections as predicted segmentations\n",
    "sample[\"predicted_segmentations_test\"] = fo.Detections(detections=detections)  # Ensure it's a list of detections\n",
    "\n",
    "# Save the updated sample\n",
    "sample.save()\n",
    "\n",
    "# Reload the dataset to reflect the changes\n",
    "dataset.reload()\n",
    "\n",
    "# Print the schema and sample to confirm the changes\n",
    "print(dataset)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec90695",
   "metadata": {
    "id": "bec90695"
   },
   "source": [
    "This loop processes each sample in the dataset by loading the image, running inference using Geti SDK, and generating instance segmentation masks. The function extracts detections with both bounding boxes and masks, ensuring they belong to the same instance. These predictions are then stored in the sample under  `\"predictions_model1\"` or `\"predictions_model2\"` using `fo.Detections`. Finally, the dataset is reloaded to reflect the updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf37a9c",
   "metadata": {
    "id": "4cf37a9c",
    "outputId": "fb0ff93e-8a43-47b7-f177-d0eb04d626c2"
   },
   "outputs": [],
   "source": [
    "# Iterate over the samples in the dataset\n",
    "for sample in dataset:\n",
    "    # Load the image as a NumPy array using PIL or OpenCV\n",
    "    image_path = sample.filepath  # Path to the image file\n",
    "    print(image_path)\n",
    "    image_data = PILImage.open(image_path)\n",
    "    image_data = np.array(image_data)  # Convert the image to NumPy array\n",
    "\n",
    "    # Run inference on the sample (using Geti SDK's inference)\n",
    "    prediction = deployment2.infer(image_data)\n",
    "\n",
    "    # Generate the segmentation mask and detections using the annotations from the prediction\n",
    "    detections = generate_mask_from_polygon_and_bboxes(sample, prediction)\n",
    "\n",
    "    # Add the detections as predicted segmentations\n",
    "    sample[\"predictions_model2\"] = fo.Detections(detections=detections)   # Change to  `\"predictions_model1\"` for evaluation\n",
    "\n",
    "    # Save the updated sample\n",
    "    sample.save()\n",
    "\n",
    "# Reload the dataset to reflect the changes\n",
    "dataset.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a8f70",
   "metadata": {
    "id": "175a8f70"
   },
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "\n",
    "Configuring Plugins in FiftyOne. To use plugins in FiftyOne, we need to enable and configure them properly.\n",
    "Minimum Configuration Steps:\n",
    "\n",
    "- Ensure FiftyOne is installed and up to date.\n",
    "- Download plugind using fiftyone CLI.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1e9d2b",
   "metadata": {
    "id": "8b1e9d2b",
    "outputId": "09a13f52-4e19-44f4-a0b9-5ce9b1668878"
   },
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c9a68",
   "metadata": {
    "id": "355c9a68"
   },
   "source": [
    "## 3. Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284334e2",
   "metadata": {
    "id": "284334e2",
    "outputId": "c1eb5b19-6bd0-4178-e7e6-2bff6305a506"
   },
   "outputs": [],
   "source": [
    "dataset.reload()\n",
    "print(dataset)\n",
    "\n",
    "# Reload the dataset to make sure we get the updated schema\n",
    "dataset.reload()\n",
    "\n",
    "# Print the dataset schema\n",
    "print(\"Dataset Schema:\", dataset.schema)\n",
    "\n",
    "# Inspect the fields of the first sample to see if 'predicted_segmentations' is added\n",
    "sample = dataset.first()\n",
    "print(\"Sample fields:\", sample.field_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d293b",
   "metadata": {
    "id": "304d293b",
    "outputId": "1a36b227-5d5e-431f-edea-efb68e0ef079"
   },
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, port=5161, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99637e8",
   "metadata": {
    "id": "a99637e8"
   },
   "source": [
    "When using the Model Evaluation Plugin in FiftyOne, we can evaluate model predictions using either:\n",
    "- The Model Evaluation Panel in the FiftyOne App ‚Äì Interactive UI-based evaluation.\n",
    "- The FiftyOne SDK ‚Äì Programmatic evaluation via Python scripts.\n",
    "\n",
    "üëâ Which one to use?\n",
    "\n",
    "- If you prefer an interactive visual analysis, the FiftyOne App provides an intuitive panel for evaluating models.\n",
    "- If you need automated and reproducible results, the FiftyOne SDK allows for batch evaluation and detailed reporting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb824db",
   "metadata": {
    "id": "afb824db",
    "outputId": "b7832200-1467-45b4-e573-a1ae9b936b28"
   },
   "outputs": [],
   "source": [
    "eval_key = \"model1_eval\"\n",
    "\n",
    "# Now evaluate on the \"defect2\" field\n",
    "eval_classif = dataset.evaluate_detections(\n",
    "    \"predictions_model1\",\n",
    "    gt_field=\"categories_segmentations\",\n",
    "    method=\"coco\", #method is important to see data in the FO app\n",
    "    classes=[\"immature\", \"semimature\", \"overmature\", \"mature\"],\n",
    "    eval_key=eval_key,  # <--- store this run under \"padim_eval\"\n",
    ")\n",
    "eval_classif.print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289ad8c",
   "metadata": {
    "id": "6289ad8c",
    "outputId": "c51e9845-42c1-48f8-acbe-5ff32ab7bfae"
   },
   "outputs": [],
   "source": [
    "eval_key_2 = \"model2_eval\"\n",
    "\n",
    "# Now evaluate on the \"defect2\" field\n",
    "eval_classif = dataset.evaluate_detections(\n",
    "    \"predictions_model2\",\n",
    "    gt_field=\"categories_segmentations\",\n",
    "    method=\"coco\", #method is important to see data in the FO app\n",
    "    classes=[\"immature\", \"semimature\", \"overmature\", \"mature\"],\n",
    "    eval_key=eval_key_2,  # <--- store this run under \"padim_eval\"\n",
    ")\n",
    "eval_classif.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d2f6b",
   "metadata": {
    "id": "e55d2f6b",
    "outputId": "3ae10137-2d4b-4faa-ad48-e674d06e4554"
   },
   "outputs": [],
   "source": [
    "new_dataset = dataset.clone()\n",
    "print(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e6e37",
   "metadata": {
    "id": "f58e6e37",
    "outputId": "be1a98c7-5665-4f0f-e0db-f6ff9ad57b3c"
   },
   "outputs": [],
   "source": [
    "export_dir = \"coffee_evaluation_FO\"\n",
    "new_dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
